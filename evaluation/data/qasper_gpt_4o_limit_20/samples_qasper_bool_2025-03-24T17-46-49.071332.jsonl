{"doc_id": 0, "doc": {"title": "pioNER: Datasets and Baselines for Armenian Named Entity Recognition", "abstract": "In this work, we tackle the problem of Armenian named entity recognition, providing silver- and gold-standard datasets as well as establishing baseline results on popular models. We present a 163000-token named entity corpus automatically generated and annotated from Wikipedia, and another 53400-token corpus of news sentences with manual annotation of people, organization and location named entities. The corpora were used to train and evaluate several popular named entity recognition models. Alongside the datasets, we release 50-, 100-, 200-, 300-dimensional GloVe word embeddings trained on a collection of Armenian texts from Wikipedia, news, blogs, and encyclopedia.", "question": "did they use a crowdsourcing platform for manual annotations?", "answer": "no", "answer_type": "bool"}, "target": "no", "arguments": {"gen_args_0": {"arg_0": ["[{\"role\": \"user\", \"content\": \"You are a strict boolean answerer. Only say 'yes' or 'no'.\\nTITLE: pioNER: Datasets and Baselines for Armenian Named Entity Recognition\\nABSTRACT: In this work, we tackle the problem of Armenian named entity recognition, providing silver- and gold-standard datasets as well as establishing baseline results on popular models. We present a 163000-token named entity corpus automatically generated and annotated from Wikipedia, and another 53400-token corpus of news sentences with manual annotation of people, organization and location named entities. The corpora were used to train and evaluate several popular named entity recognition models. Alongside the datasets, we release 50-, 100-, 200-, 300-dimensional GloVe word embeddings trained on a collection of Armenian texts from Wikipedia, news, blogs, and encyclopedia.\\nQUESTION: did they use a crowdsourcing platform for manual annotations?\\nANSWER:\"}]"], "arg_1": {"until": ["\n", "."], "max_gen_toks": 5}}}, "resps": [["No"]], "filtered_resps": ["no"], "filter": "get-answer", "metrics": ["exact_match"], "doc_hash": "135073997ac52da5b4801101d9e523ff4e57e54c3c7757174ca5e1c5fa2988db", "prompt_hash": "5ca1785f9efae13d85cda3acddd540a9b169adcdc6c4bb48c7c044d8eac78e7c", "target_hash": "9390298f3fb0c5b160498935d79cb139aef28e1c47358b4bbba61862b9c26e59", "exact_match": 1.0}
{"doc_id": 1, "doc": {"title": "pioNER: Datasets and Baselines for Armenian Named Entity Recognition", "abstract": "In this work, we tackle the problem of Armenian named entity recognition, providing silver- and gold-standard datasets as well as establishing baseline results on popular models. We present a 163000-token named entity corpus automatically generated and annotated from Wikipedia, and another 53400-token corpus of news sentences with manual annotation of people, organization and location named entities. The corpora were used to train and evaluate several popular named entity recognition models. Alongside the datasets, we release 50-, 100-, 200-, 300-dimensional GloVe word embeddings trained on a collection of Armenian texts from Wikipedia, news, blogs, and encyclopedia.", "question": "did they use a crowdsourcing platform for manual annotations?", "answer": "no", "answer_type": "bool"}, "target": "no", "arguments": {"gen_args_0": {"arg_0": ["[{\"role\": \"user\", \"content\": \"You are a strict boolean answerer. Only say 'yes' or 'no'.\\nTITLE: pioNER: Datasets and Baselines for Armenian Named Entity Recognition\\nABSTRACT: In this work, we tackle the problem of Armenian named entity recognition, providing silver- and gold-standard datasets as well as establishing baseline results on popular models. We present a 163000-token named entity corpus automatically generated and annotated from Wikipedia, and another 53400-token corpus of news sentences with manual annotation of people, organization and location named entities. The corpora were used to train and evaluate several popular named entity recognition models. Alongside the datasets, we release 50-, 100-, 200-, 300-dimensional GloVe word embeddings trained on a collection of Armenian texts from Wikipedia, news, blogs, and encyclopedia.\\nQUESTION: did they use a crowdsourcing platform for manual annotations?\\nANSWER:\"}]"], "arg_1": {"until": ["\n", "."], "max_gen_toks": 5}}}, "resps": [["No"]], "filtered_resps": ["no"], "filter": "get-answer", "metrics": ["exact_match"], "doc_hash": "135073997ac52da5b4801101d9e523ff4e57e54c3c7757174ca5e1c5fa2988db", "prompt_hash": "5ca1785f9efae13d85cda3acddd540a9b169adcdc6c4bb48c7c044d8eac78e7c", "target_hash": "9390298f3fb0c5b160498935d79cb139aef28e1c47358b4bbba61862b9c26e59", "exact_match": 1.0}
{"doc_id": 2, "doc": {"title": "Sensitive Data Detection and Classification in Spanish Clinical Text: Experiments with BERT", "abstract": "Massive digital data processing provides a wide range of opportunities and benefits, but at the cost of endangering personal data privacy. Anonymisation consists in removing or replacing sensitive information from data, enabling its exploitation for different purposes while preserving the privacy of individuals. Over the years, a lot of automatic anonymisation systems have been proposed; however, depending on the type of data, the target language or the availability of training documents, the task remains challenging still. The emergence of novel deep-learning models during the last two years has brought large improvements to the state of the art in the field of Natural Language Processing. These advancements have been most noticeably led by BERT, a model proposed by Google in 2018, and the shared language models pre-trained on millions of documents. In this paper, we use a BERT-based sequence labelling model to conduct a series of anonymisation experiments on several clinical datasets in Spanish. We also compare BERT to other algorithms. The experiments show that a simple BERT-based model with general-domain pre-training obtains highly competitive results without any domain specific feature engineering.", "question": "Does BERT reach the best performance among all the algorithms compared?", "answer": "no", "answer_type": "bool"}, "target": "no", "arguments": {"gen_args_0": {"arg_0": ["[{\"role\": \"user\", \"content\": \"You are a strict boolean answerer. Only say 'yes' or 'no'.\\nTITLE: Sensitive Data Detection and Classification in Spanish Clinical Text: Experiments with BERT\\nABSTRACT: Massive digital data processing provides a wide range of opportunities and benefits, but at the cost of endangering personal data privacy. Anonymisation consists in removing or replacing sensitive information from data, enabling its exploitation for different purposes while preserving the privacy of individuals. Over the years, a lot of automatic anonymisation systems have been proposed; however, depending on the type of data, the target language or the availability of training documents, the task remains challenging still. The emergence of novel deep-learning models during the last two years has brought large improvements to the state of the art in the field of Natural Language Processing. These advancements have been most noticeably led by BERT, a model proposed by Google in 2018, and the shared language models pre-trained on millions of documents. In this paper, we use a BERT-based sequence labelling model to conduct a series of anonymisation experiments on several clinical datasets in Spanish. We also compare BERT to other algorithms. The experiments show that a simple BERT-based model with general-domain pre-training obtains highly competitive results without any domain specific feature engineering.\\nQUESTION: Does BERT reach the best performance among all the algorithms compared?\\nANSWER:\"}]"], "arg_1": {"until": ["\n", "."], "max_gen_toks": 5}}}, "resps": [["Yes"]], "filtered_resps": ["yes"], "filter": "get-answer", "metrics": ["exact_match"], "doc_hash": "0d1e78a6bfdc5e2e3617caa7e932911534cad07fbb0a7c02c66f2808b4c2ab1d", "prompt_hash": "ab756a4adc84e8ade0171e600c56b03bee8fec6bea74a181be9167fb6ea1cdc2", "target_hash": "9390298f3fb0c5b160498935d79cb139aef28e1c47358b4bbba61862b9c26e59", "exact_match": 0.0}
{"doc_id": 3, "doc": {"title": "Sensitive Data Detection and Classification in Spanish Clinical Text: Experiments with BERT", "abstract": "Massive digital data processing provides a wide range of opportunities and benefits, but at the cost of endangering personal data privacy. Anonymisation consists in removing or replacing sensitive information from data, enabling its exploitation for different purposes while preserving the privacy of individuals. Over the years, a lot of automatic anonymisation systems have been proposed; however, depending on the type of data, the target language or the availability of training documents, the task remains challenging still. The emergence of novel deep-learning models during the last two years has brought large improvements to the state of the art in the field of Natural Language Processing. These advancements have been most noticeably led by BERT, a model proposed by Google in 2018, and the shared language models pre-trained on millions of documents. In this paper, we use a BERT-based sequence labelling model to conduct a series of anonymisation experiments on several clinical datasets in Spanish. We also compare BERT to other algorithms. The experiments show that a simple BERT-based model with general-domain pre-training obtains highly competitive results without any domain specific feature engineering.", "question": "Does BERT reach the best performance among all the algorithms compared?", "answer": "no", "answer_type": "bool"}, "target": "no", "arguments": {"gen_args_0": {"arg_0": ["[{\"role\": \"user\", \"content\": \"You are a strict boolean answerer. Only say 'yes' or 'no'.\\nTITLE: Sensitive Data Detection and Classification in Spanish Clinical Text: Experiments with BERT\\nABSTRACT: Massive digital data processing provides a wide range of opportunities and benefits, but at the cost of endangering personal data privacy. Anonymisation consists in removing or replacing sensitive information from data, enabling its exploitation for different purposes while preserving the privacy of individuals. Over the years, a lot of automatic anonymisation systems have been proposed; however, depending on the type of data, the target language or the availability of training documents, the task remains challenging still. The emergence of novel deep-learning models during the last two years has brought large improvements to the state of the art in the field of Natural Language Processing. These advancements have been most noticeably led by BERT, a model proposed by Google in 2018, and the shared language models pre-trained on millions of documents. In this paper, we use a BERT-based sequence labelling model to conduct a series of anonymisation experiments on several clinical datasets in Spanish. We also compare BERT to other algorithms. The experiments show that a simple BERT-based model with general-domain pre-training obtains highly competitive results without any domain specific feature engineering.\\nQUESTION: Does BERT reach the best performance among all the algorithms compared?\\nANSWER:\"}]"], "arg_1": {"until": ["\n", "."], "max_gen_toks": 5}}}, "resps": [["Yes"]], "filtered_resps": ["yes"], "filter": "get-answer", "metrics": ["exact_match"], "doc_hash": "0d1e78a6bfdc5e2e3617caa7e932911534cad07fbb0a7c02c66f2808b4c2ab1d", "prompt_hash": "ab756a4adc84e8ade0171e600c56b03bee8fec6bea74a181be9167fb6ea1cdc2", "target_hash": "9390298f3fb0c5b160498935d79cb139aef28e1c47358b4bbba61862b9c26e59", "exact_match": 0.0}
{"doc_id": 4, "doc": {"title": "Abstractive Summarization for Low Resource Data using Domain Transfer and Data Synthesis", "abstract": "Training abstractive summarization models typically requires large amounts of data, which can be a limitation for many domains. In this paper we explore using domain transfer and data synthesis to improve the performance of recent abstractive summarization methods when applied to small corpora of student reflections. First, we explored whether tuning state of the art model trained on newspaper data could boost performance on student reflection data. Evaluations demonstrated that summaries produced by the tuned model achieved higher ROUGE scores compared to model trained on just student reflection data or just newspaper data. The tuned model also achieved higher scores compared to extractive summarization baselines, and additionally was judged to produce more coherent and readable summaries in human evaluations. Second, we explored whether synthesizing summaries of student data could additionally boost performance. We proposed a template-based model to synthesize new data, which when incorporated into training further increased ROUGE scores. Finally, we showed that combining data synthesis with domain transfer achieved higher ROUGE scores compared to only using one of the two approaches.", "question": "Is the template-based model realistic?  ", "answer": "yes", "answer_type": "bool"}, "target": "yes", "arguments": {"gen_args_0": {"arg_0": ["[{\"role\": \"user\", \"content\": \"You are a strict boolean answerer. Only say 'yes' or 'no'.\\nTITLE: Abstractive Summarization for Low Resource Data using Domain Transfer and Data Synthesis\\nABSTRACT: Training abstractive summarization models typically requires large amounts of data, which can be a limitation for many domains. In this paper we explore using domain transfer and data synthesis to improve the performance of recent abstractive summarization methods when applied to small corpora of student reflections. First, we explored whether tuning state of the art model trained on newspaper data could boost performance on student reflection data. Evaluations demonstrated that summaries produced by the tuned model achieved higher ROUGE scores compared to model trained on just student reflection data or just newspaper data. The tuned model also achieved higher scores compared to extractive summarization baselines, and additionally was judged to produce more coherent and readable summaries in human evaluations. Second, we explored whether synthesizing summaries of student data could additionally boost performance. We proposed a template-based model to synthesize new data, which when incorporated into training further increased ROUGE scores. Finally, we showed that combining data synthesis with domain transfer achieved higher ROUGE scores compared to only using one of the two approaches.\\nQUESTION: Is the template-based model realistic?  \\nANSWER:\"}]"], "arg_1": {"until": ["\n", "."], "max_gen_toks": 5}}}, "resps": [["No"]], "filtered_resps": ["no"], "filter": "get-answer", "metrics": ["exact_match"], "doc_hash": "c8506c80c85e0d4842b0076b9d97fbb1a3e3ff4c763a93c5f7de45c860d3ff78", "prompt_hash": "e9d0179c9d2176b3900fc52ff8c7cd9ef3f296b7951f47f36f284b59cc7257d5", "target_hash": "8a798890fe93817163b10b5f7bd2ca4d25d84c52739a645a889c173eee7d9d3d", "exact_match": 0.0}
{"doc_id": 5, "doc": {"title": "Abstractive Summarization for Low Resource Data using Domain Transfer and Data Synthesis", "abstract": "Training abstractive summarization models typically requires large amounts of data, which can be a limitation for many domains. In this paper we explore using domain transfer and data synthesis to improve the performance of recent abstractive summarization methods when applied to small corpora of student reflections. First, we explored whether tuning state of the art model trained on newspaper data could boost performance on student reflection data. Evaluations demonstrated that summaries produced by the tuned model achieved higher ROUGE scores compared to model trained on just student reflection data or just newspaper data. The tuned model also achieved higher scores compared to extractive summarization baselines, and additionally was judged to produce more coherent and readable summaries in human evaluations. Second, we explored whether synthesizing summaries of student data could additionally boost performance. We proposed a template-based model to synthesize new data, which when incorporated into training further increased ROUGE scores. Finally, we showed that combining data synthesis with domain transfer achieved higher ROUGE scores compared to only using one of the two approaches.", "question": "Is the template-based model realistic?  ", "answer": "yes", "answer_type": "bool"}, "target": "yes", "arguments": {"gen_args_0": {"arg_0": ["[{\"role\": \"user\", \"content\": \"You are a strict boolean answerer. Only say 'yes' or 'no'.\\nTITLE: Abstractive Summarization for Low Resource Data using Domain Transfer and Data Synthesis\\nABSTRACT: Training abstractive summarization models typically requires large amounts of data, which can be a limitation for many domains. In this paper we explore using domain transfer and data synthesis to improve the performance of recent abstractive summarization methods when applied to small corpora of student reflections. First, we explored whether tuning state of the art model trained on newspaper data could boost performance on student reflection data. Evaluations demonstrated that summaries produced by the tuned model achieved higher ROUGE scores compared to model trained on just student reflection data or just newspaper data. The tuned model also achieved higher scores compared to extractive summarization baselines, and additionally was judged to produce more coherent and readable summaries in human evaluations. Second, we explored whether synthesizing summaries of student data could additionally boost performance. We proposed a template-based model to synthesize new data, which when incorporated into training further increased ROUGE scores. Finally, we showed that combining data synthesis with domain transfer achieved higher ROUGE scores compared to only using one of the two approaches.\\nQUESTION: Is the template-based model realistic?  \\nANSWER:\"}]"], "arg_1": {"until": ["\n", "."], "max_gen_toks": 5}}}, "resps": [["No"]], "filtered_resps": ["no"], "filter": "get-answer", "metrics": ["exact_match"], "doc_hash": "c8506c80c85e0d4842b0076b9d97fbb1a3e3ff4c763a93c5f7de45c860d3ff78", "prompt_hash": "e9d0179c9d2176b3900fc52ff8c7cd9ef3f296b7951f47f36f284b59cc7257d5", "target_hash": "8a798890fe93817163b10b5f7bd2ca4d25d84c52739a645a889c173eee7d9d3d", "exact_match": 0.0}
{"doc_id": 6, "doc": {"title": "Abstractive Summarization for Low Resource Data using Domain Transfer and Data Synthesis", "abstract": "Training abstractive summarization models typically requires large amounts of data, which can be a limitation for many domains. In this paper we explore using domain transfer and data synthesis to improve the performance of recent abstractive summarization methods when applied to small corpora of student reflections. First, we explored whether tuning state of the art model trained on newspaper data could boost performance on student reflection data. Evaluations demonstrated that summaries produced by the tuned model achieved higher ROUGE scores compared to model trained on just student reflection data or just newspaper data. The tuned model also achieved higher scores compared to extractive summarization baselines, and additionally was judged to produce more coherent and readable summaries in human evaluations. Second, we explored whether synthesizing summaries of student data could additionally boost performance. We proposed a template-based model to synthesize new data, which when incorporated into training further increased ROUGE scores. Finally, we showed that combining data synthesis with domain transfer achieved higher ROUGE scores compared to only using one of the two approaches.", "question": "Is the student reflection data very different from the newspaper data?  ", "answer": "yes", "answer_type": "bool"}, "target": "yes", "arguments": {"gen_args_0": {"arg_0": ["[{\"role\": \"user\", \"content\": \"You are a strict boolean answerer. Only say 'yes' or 'no'.\\nTITLE: Abstractive Summarization for Low Resource Data using Domain Transfer and Data Synthesis\\nABSTRACT: Training abstractive summarization models typically requires large amounts of data, which can be a limitation for many domains. In this paper we explore using domain transfer and data synthesis to improve the performance of recent abstractive summarization methods when applied to small corpora of student reflections. First, we explored whether tuning state of the art model trained on newspaper data could boost performance on student reflection data. Evaluations demonstrated that summaries produced by the tuned model achieved higher ROUGE scores compared to model trained on just student reflection data or just newspaper data. The tuned model also achieved higher scores compared to extractive summarization baselines, and additionally was judged to produce more coherent and readable summaries in human evaluations. Second, we explored whether synthesizing summaries of student data could additionally boost performance. We proposed a template-based model to synthesize new data, which when incorporated into training further increased ROUGE scores. Finally, we showed that combining data synthesis with domain transfer achieved higher ROUGE scores compared to only using one of the two approaches.\\nQUESTION: Is the student reflection data very different from the newspaper data?  \\nANSWER:\"}]"], "arg_1": {"until": ["\n", "."], "max_gen_toks": 5}}}, "resps": [["Yes"]], "filtered_resps": ["yes"], "filter": "get-answer", "metrics": ["exact_match"], "doc_hash": "f82c3358e8257fc494540f745613a57cc5f2e7adc61cba756379061e86d34efa", "prompt_hash": "15e72c2e6bc5510ef683182fe8e37cd3420da9eb73f2c9d84e7586da24670793", "target_hash": "8a798890fe93817163b10b5f7bd2ca4d25d84c52739a645a889c173eee7d9d3d", "exact_match": 1.0}
{"doc_id": 7, "doc": {"title": "Abstractive Summarization for Low Resource Data using Domain Transfer and Data Synthesis", "abstract": "Training abstractive summarization models typically requires large amounts of data, which can be a limitation for many domains. In this paper we explore using domain transfer and data synthesis to improve the performance of recent abstractive summarization methods when applied to small corpora of student reflections. First, we explored whether tuning state of the art model trained on newspaper data could boost performance on student reflection data. Evaluations demonstrated that summaries produced by the tuned model achieved higher ROUGE scores compared to model trained on just student reflection data or just newspaper data. The tuned model also achieved higher scores compared to extractive summarization baselines, and additionally was judged to produce more coherent and readable summaries in human evaluations. Second, we explored whether synthesizing summaries of student data could additionally boost performance. We proposed a template-based model to synthesize new data, which when incorporated into training further increased ROUGE scores. Finally, we showed that combining data synthesis with domain transfer achieved higher ROUGE scores compared to only using one of the two approaches.", "question": "Is the student reflection data very different from the newspaper data?  ", "answer": "yes", "answer_type": "bool"}, "target": "yes", "arguments": {"gen_args_0": {"arg_0": ["[{\"role\": \"user\", \"content\": \"You are a strict boolean answerer. Only say 'yes' or 'no'.\\nTITLE: Abstractive Summarization for Low Resource Data using Domain Transfer and Data Synthesis\\nABSTRACT: Training abstractive summarization models typically requires large amounts of data, which can be a limitation for many domains. In this paper we explore using domain transfer and data synthesis to improve the performance of recent abstractive summarization methods when applied to small corpora of student reflections. First, we explored whether tuning state of the art model trained on newspaper data could boost performance on student reflection data. Evaluations demonstrated that summaries produced by the tuned model achieved higher ROUGE scores compared to model trained on just student reflection data or just newspaper data. The tuned model also achieved higher scores compared to extractive summarization baselines, and additionally was judged to produce more coherent and readable summaries in human evaluations. Second, we explored whether synthesizing summaries of student data could additionally boost performance. We proposed a template-based model to synthesize new data, which when incorporated into training further increased ROUGE scores. Finally, we showed that combining data synthesis with domain transfer achieved higher ROUGE scores compared to only using one of the two approaches.\\nQUESTION: Is the student reflection data very different from the newspaper data?  \\nANSWER:\"}]"], "arg_1": {"until": ["\n", "."], "max_gen_toks": 5}}}, "resps": [["Yes"]], "filtered_resps": ["yes"], "filter": "get-answer", "metrics": ["exact_match"], "doc_hash": "f82c3358e8257fc494540f745613a57cc5f2e7adc61cba756379061e86d34efa", "prompt_hash": "15e72c2e6bc5510ef683182fe8e37cd3420da9eb73f2c9d84e7586da24670793", "target_hash": "8a798890fe93817163b10b5f7bd2ca4d25d84c52739a645a889c173eee7d9d3d", "exact_match": 1.0}
{"doc_id": 8, "doc": {"title": "Assessing BERT's Syntactic Abilities", "abstract": "I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2)\"coloreless green ideas\"subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases.", "question": "Were any of these tasks evaluated in any previous work?", "answer": "yes", "answer_type": "bool"}, "target": "yes", "arguments": {"gen_args_0": {"arg_0": ["[{\"role\": \"user\", \"content\": \"You are a strict boolean answerer. Only say 'yes' or 'no'.\\nTITLE: Assessing BERT's Syntactic Abilities\\nABSTRACT: I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2)\\\"coloreless green ideas\\\"subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases.\\nQUESTION: Were any of these tasks evaluated in any previous work?\\nANSWER:\"}]"], "arg_1": {"until": ["\n", "."], "max_gen_toks": 5}}}, "resps": [["No"]], "filtered_resps": ["no"], "filter": "get-answer", "metrics": ["exact_match"], "doc_hash": "6e6c2a876a8f5826b2b6b54897ad26489baef74a2b89fbc2637b1fbf02a0253e", "prompt_hash": "a92a1b3f29a64374fadf54334681b4006c87aab9640b7c5d86f9151342f655a2", "target_hash": "8a798890fe93817163b10b5f7bd2ca4d25d84c52739a645a889c173eee7d9d3d", "exact_match": 0.0}
{"doc_id": 9, "doc": {"title": "Assessing BERT's Syntactic Abilities", "abstract": "I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2)\"coloreless green ideas\"subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases.", "question": "Were any of these tasks evaluated in any previous work?", "answer": "yes", "answer_type": "bool"}, "target": "yes", "arguments": {"gen_args_0": {"arg_0": ["[{\"role\": \"user\", \"content\": \"You are a strict boolean answerer. Only say 'yes' or 'no'.\\nTITLE: Assessing BERT's Syntactic Abilities\\nABSTRACT: I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2)\\\"coloreless green ideas\\\"subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases.\\nQUESTION: Were any of these tasks evaluated in any previous work?\\nANSWER:\"}]"], "arg_1": {"until": ["\n", "."], "max_gen_toks": 5}}}, "resps": [["No"]], "filtered_resps": ["no"], "filter": "get-answer", "metrics": ["exact_match"], "doc_hash": "6e6c2a876a8f5826b2b6b54897ad26489baef74a2b89fbc2637b1fbf02a0253e", "prompt_hash": "a92a1b3f29a64374fadf54334681b4006c87aab9640b7c5d86f9151342f655a2", "target_hash": "8a798890fe93817163b10b5f7bd2ca4d25d84c52739a645a889c173eee7d9d3d", "exact_match": 0.0}
{"doc_id": 10, "doc": {"title": "Stateology: State-Level Interactive Charting of Language, Feelings, and Values", "abstract": "People's personality and motivations are manifest in their everyday language usage. With the emergence of social media, ample examples of such usage are procurable. In this paper, we aim to analyze the vocabulary used by close to 200,000 Blogger users in the U.S. with the purpose of geographically portraying various demographic, linguistic, and psychological dimensions at the state level. We give a description of a web-based tool for viewing maps that depict various characteristics of the social media users as derived from this large blog dataset of over two billion words.", "question": "Do they build a model to automatically detect demographic, lingustic or psycological dimensons of people?", "answer": "no", "answer_type": "bool"}, "target": "no", "arguments": {"gen_args_0": {"arg_0": ["[{\"role\": \"user\", \"content\": \"You are a strict boolean answerer. Only say 'yes' or 'no'.\\nTITLE: Stateology: State-Level Interactive Charting of Language, Feelings, and Values\\nABSTRACT: People's personality and motivations are manifest in their everyday language usage. With the emergence of social media, ample examples of such usage are procurable. In this paper, we aim to analyze the vocabulary used by close to 200,000 Blogger users in the U.S. with the purpose of geographically portraying various demographic, linguistic, and psychological dimensions at the state level. We give a description of a web-based tool for viewing maps that depict various characteristics of the social media users as derived from this large blog dataset of over two billion words.\\nQUESTION: Do they build a model to automatically detect demographic, lingustic or psycological dimensons of people?\\nANSWER:\"}]"], "arg_1": {"until": ["\n", "."], "max_gen_toks": 5}}}, "resps": [["No"]], "filtered_resps": ["no"], "filter": "get-answer", "metrics": ["exact_match"], "doc_hash": "c1accaad4eb6cacc6b5f2384a55a0c3362ab67b50e1708ef71a019d4853ee6ae", "prompt_hash": "442c5a3ec65e214acac485bb361f94eeb26584e0dde09c04681fab4afb1b00ac", "target_hash": "9390298f3fb0c5b160498935d79cb139aef28e1c47358b4bbba61862b9c26e59", "exact_match": 1.0}
{"doc_id": 11, "doc": {"title": "Stateology: State-Level Interactive Charting of Language, Feelings, and Values", "abstract": "People's personality and motivations are manifest in their everyday language usage. With the emergence of social media, ample examples of such usage are procurable. In this paper, we aim to analyze the vocabulary used by close to 200,000 Blogger users in the U.S. with the purpose of geographically portraying various demographic, linguistic, and psychological dimensions at the state level. We give a description of a web-based tool for viewing maps that depict various characteristics of the social media users as derived from this large blog dataset of over two billion words.", "question": "Do they build a model to automatically detect demographic, lingustic or psycological dimensons of people?", "answer": "no", "answer_type": "bool"}, "target": "no", "arguments": {"gen_args_0": {"arg_0": ["[{\"role\": \"user\", \"content\": \"You are a strict boolean answerer. Only say 'yes' or 'no'.\\nTITLE: Stateology: State-Level Interactive Charting of Language, Feelings, and Values\\nABSTRACT: People's personality and motivations are manifest in their everyday language usage. With the emergence of social media, ample examples of such usage are procurable. In this paper, we aim to analyze the vocabulary used by close to 200,000 Blogger users in the U.S. with the purpose of geographically portraying various demographic, linguistic, and psychological dimensions at the state level. We give a description of a web-based tool for viewing maps that depict various characteristics of the social media users as derived from this large blog dataset of over two billion words.\\nQUESTION: Do they build a model to automatically detect demographic, lingustic or psycological dimensons of people?\\nANSWER:\"}]"], "arg_1": {"until": ["\n", "."], "max_gen_toks": 5}}}, "resps": [["No"]], "filtered_resps": ["no"], "filter": "get-answer", "metrics": ["exact_match"], "doc_hash": "c1accaad4eb6cacc6b5f2384a55a0c3362ab67b50e1708ef71a019d4853ee6ae", "prompt_hash": "442c5a3ec65e214acac485bb361f94eeb26584e0dde09c04681fab4afb1b00ac", "target_hash": "9390298f3fb0c5b160498935d79cb139aef28e1c47358b4bbba61862b9c26e59", "exact_match": 1.0}
{"doc_id": 12, "doc": {"title": "Medication Regimen Extraction From Clinical Conversations", "abstract": "Extracting relevant information from clinical conversations and providing it to doctors and patients might help in addressing doctor burnout and patient forgetfulness. In this paper, we focus on extracting the Medication Regimen (dosage and frequency for medications) discussed in a clinical conversation. We frame the problem as a Question Answering (QA) task and perform comparative analysis over: a QA approach, a new combined QA and Information Extraction approach and other baselines. We use a small corpus of 6,692 annotated doctor-patient conversations for the task. Clinical conversation corpora are costly to create, difficult to handle (because of data privacy concerns), and thus `scarce'. We address this data scarcity challenge through data augmentation methods, using publicly available embeddings and pretrain part of the network on a related task of summarization to improve the model's performance. Compared to the baseline, our best-performing models improve the dosage and frequency extractions' ROUGE-1 F1 scores from 54.28 and 37.13 to 89.57 and 45.94, respectively. Using our best-performing model, we present the first fully automated system that can extract Medication Regimen (MR) tags from spontaneous doctor-patient conversations with about ~71% accuracy.", "question": "Is the data de-identified?", "answer": "yes", "answer_type": "bool"}, "target": "yes", "arguments": {"gen_args_0": {"arg_0": ["[{\"role\": \"user\", \"content\": \"You are a strict boolean answerer. Only say 'yes' or 'no'.\\nTITLE: Medication Regimen Extraction From Clinical Conversations\\nABSTRACT: Extracting relevant information from clinical conversations and providing it to doctors and patients might help in addressing doctor burnout and patient forgetfulness. In this paper, we focus on extracting the Medication Regimen (dosage and frequency for medications) discussed in a clinical conversation. We frame the problem as a Question Answering (QA) task and perform comparative analysis over: a QA approach, a new combined QA and Information Extraction approach and other baselines. We use a small corpus of 6,692 annotated doctor-patient conversations for the task. Clinical conversation corpora are costly to create, difficult to handle (because of data privacy concerns), and thus `scarce'. We address this data scarcity challenge through data augmentation methods, using publicly available embeddings and pretrain part of the network on a related task of summarization to improve the model's performance. Compared to the baseline, our best-performing models improve the dosage and frequency extractions' ROUGE-1 F1 scores from 54.28 and 37.13 to 89.57 and 45.94, respectively. Using our best-performing model, we present the first fully automated system that can extract Medication Regimen (MR) tags from spontaneous doctor-patient conversations with about ~71% accuracy.\\nQUESTION: Is the data de-identified?\\nANSWER:\"}]"], "arg_1": {"until": ["\n", "."], "max_gen_toks": 5}}}, "resps": [["No"]], "filtered_resps": ["no"], "filter": "get-answer", "metrics": ["exact_match"], "doc_hash": "a15bae36dcaef9986564c9dc3c3969ee617e02e569de66e97a0dea5d12d64c17", "prompt_hash": "02d463320929cf4a33482be0c00e6d22376259b400ae7cc897673424a903ca79", "target_hash": "8a798890fe93817163b10b5f7bd2ca4d25d84c52739a645a889c173eee7d9d3d", "exact_match": 0.0}
{"doc_id": 13, "doc": {"title": "Medication Regimen Extraction From Clinical Conversations", "abstract": "Extracting relevant information from clinical conversations and providing it to doctors and patients might help in addressing doctor burnout and patient forgetfulness. In this paper, we focus on extracting the Medication Regimen (dosage and frequency for medications) discussed in a clinical conversation. We frame the problem as a Question Answering (QA) task and perform comparative analysis over: a QA approach, a new combined QA and Information Extraction approach and other baselines. We use a small corpus of 6,692 annotated doctor-patient conversations for the task. Clinical conversation corpora are costly to create, difficult to handle (because of data privacy concerns), and thus `scarce'. We address this data scarcity challenge through data augmentation methods, using publicly available embeddings and pretrain part of the network on a related task of summarization to improve the model's performance. Compared to the baseline, our best-performing models improve the dosage and frequency extractions' ROUGE-1 F1 scores from 54.28 and 37.13 to 89.57 and 45.94, respectively. Using our best-performing model, we present the first fully automated system that can extract Medication Regimen (MR) tags from spontaneous doctor-patient conversations with about ~71% accuracy.", "question": "Is the data de-identified?", "answer": "yes", "answer_type": "bool"}, "target": "yes", "arguments": {"gen_args_0": {"arg_0": ["[{\"role\": \"user\", \"content\": \"You are a strict boolean answerer. Only say 'yes' or 'no'.\\nTITLE: Medication Regimen Extraction From Clinical Conversations\\nABSTRACT: Extracting relevant information from clinical conversations and providing it to doctors and patients might help in addressing doctor burnout and patient forgetfulness. In this paper, we focus on extracting the Medication Regimen (dosage and frequency for medications) discussed in a clinical conversation. We frame the problem as a Question Answering (QA) task and perform comparative analysis over: a QA approach, a new combined QA and Information Extraction approach and other baselines. We use a small corpus of 6,692 annotated doctor-patient conversations for the task. Clinical conversation corpora are costly to create, difficult to handle (because of data privacy concerns), and thus `scarce'. We address this data scarcity challenge through data augmentation methods, using publicly available embeddings and pretrain part of the network on a related task of summarization to improve the model's performance. Compared to the baseline, our best-performing models improve the dosage and frequency extractions' ROUGE-1 F1 scores from 54.28 and 37.13 to 89.57 and 45.94, respectively. Using our best-performing model, we present the first fully automated system that can extract Medication Regimen (MR) tags from spontaneous doctor-patient conversations with about ~71% accuracy.\\nQUESTION: Is the data de-identified?\\nANSWER:\"}]"], "arg_1": {"until": ["\n", "."], "max_gen_toks": 5}}}, "resps": [["No"]], "filtered_resps": ["no"], "filter": "get-answer", "metrics": ["exact_match"], "doc_hash": "a15bae36dcaef9986564c9dc3c3969ee617e02e569de66e97a0dea5d12d64c17", "prompt_hash": "02d463320929cf4a33482be0c00e6d22376259b400ae7cc897673424a903ca79", "target_hash": "8a798890fe93817163b10b5f7bd2ca4d25d84c52739a645a889c173eee7d9d3d", "exact_match": 0.0}
{"doc_id": 14, "doc": {"title": "Multipurpose Intelligent Process Automation via Conversational Assistant", "abstract": "Intelligent Process Automation (IPA) is an emerging technology with a primary goal to assist the knowledge worker by taking care of repetitive, routine and low-cognitive tasks. Conversational agents that can interact with users in a natural language are potential application for IPA systems. Such intelligent agents can assist the user by answering specific questions and executing routine tasks that are ordinarily performed in a natural language (i.e., customer support). In this work, we tackle a challenge of implementing an IPA conversational assistant in a real-world industrial setting with a lack of structured training data. Our proposed system brings two significant benefits: First, it reduces repetitive and time-consuming activities and, therefore, allows workers to focus on more intelligent processes. Second, by interacting with users, it augments the resources with structured and to some extent labeled training data. We showcase the usage of the latter by re-implementing several components of our system with Transfer Learning (TL) methods.", "question": "Do they use off-the-shelf NLP systems to build their assitant?", "answer": "no", "answer_type": "bool"}, "target": "no", "arguments": {"gen_args_0": {"arg_0": ["[{\"role\": \"user\", \"content\": \"You are a strict boolean answerer. Only say 'yes' or 'no'.\\nTITLE: Multipurpose Intelligent Process Automation via Conversational Assistant\\nABSTRACT: Intelligent Process Automation (IPA) is an emerging technology with a primary goal to assist the knowledge worker by taking care of repetitive, routine and low-cognitive tasks. Conversational agents that can interact with users in a natural language are potential application for IPA systems. Such intelligent agents can assist the user by answering specific questions and executing routine tasks that are ordinarily performed in a natural language (i.e., customer support). In this work, we tackle a challenge of implementing an IPA conversational assistant in a real-world industrial setting with a lack of structured training data. Our proposed system brings two significant benefits: First, it reduces repetitive and time-consuming activities and, therefore, allows workers to focus on more intelligent processes. Second, by interacting with users, it augments the resources with structured and to some extent labeled training data. We showcase the usage of the latter by re-implementing several components of our system with Transfer Learning (TL) methods.\\nQUESTION: Do they use off-the-shelf NLP systems to build their assitant?\\nANSWER:\"}]"], "arg_1": {"until": ["\n", "."], "max_gen_toks": 5}}}, "resps": [["No"]], "filtered_resps": ["no"], "filter": "get-answer", "metrics": ["exact_match"], "doc_hash": "125c0bc0e440fef9d8f41a88a29d3fc3f9a62b26e7f3a9eb959d9524f5262730", "prompt_hash": "4ccbd5c28d318992a76545deb6587e21326198d84254e1d9f3999796e6a673d4", "target_hash": "9390298f3fb0c5b160498935d79cb139aef28e1c47358b4bbba61862b9c26e59", "exact_match": 1.0}
{"doc_id": 15, "doc": {"title": "Multipurpose Intelligent Process Automation via Conversational Assistant", "abstract": "Intelligent Process Automation (IPA) is an emerging technology with a primary goal to assist the knowledge worker by taking care of repetitive, routine and low-cognitive tasks. Conversational agents that can interact with users in a natural language are potential application for IPA systems. Such intelligent agents can assist the user by answering specific questions and executing routine tasks that are ordinarily performed in a natural language (i.e., customer support). In this work, we tackle a challenge of implementing an IPA conversational assistant in a real-world industrial setting with a lack of structured training data. Our proposed system brings two significant benefits: First, it reduces repetitive and time-consuming activities and, therefore, allows workers to focus on more intelligent processes. Second, by interacting with users, it augments the resources with structured and to some extent labeled training data. We showcase the usage of the latter by re-implementing several components of our system with Transfer Learning (TL) methods.", "question": "Do they use off-the-shelf NLP systems to build their assitant?", "answer": "no", "answer_type": "bool"}, "target": "no", "arguments": {"gen_args_0": {"arg_0": ["[{\"role\": \"user\", \"content\": \"You are a strict boolean answerer. Only say 'yes' or 'no'.\\nTITLE: Multipurpose Intelligent Process Automation via Conversational Assistant\\nABSTRACT: Intelligent Process Automation (IPA) is an emerging technology with a primary goal to assist the knowledge worker by taking care of repetitive, routine and low-cognitive tasks. Conversational agents that can interact with users in a natural language are potential application for IPA systems. Such intelligent agents can assist the user by answering specific questions and executing routine tasks that are ordinarily performed in a natural language (i.e., customer support). In this work, we tackle a challenge of implementing an IPA conversational assistant in a real-world industrial setting with a lack of structured training data. Our proposed system brings two significant benefits: First, it reduces repetitive and time-consuming activities and, therefore, allows workers to focus on more intelligent processes. Second, by interacting with users, it augments the resources with structured and to some extent labeled training data. We showcase the usage of the latter by re-implementing several components of our system with Transfer Learning (TL) methods.\\nQUESTION: Do they use off-the-shelf NLP systems to build their assitant?\\nANSWER:\"}]"], "arg_1": {"until": ["\n", "."], "max_gen_toks": 5}}}, "resps": [["No"]], "filtered_resps": ["no"], "filter": "get-answer", "metrics": ["exact_match"], "doc_hash": "125c0bc0e440fef9d8f41a88a29d3fc3f9a62b26e7f3a9eb959d9524f5262730", "prompt_hash": "4ccbd5c28d318992a76545deb6587e21326198d84254e1d9f3999796e6a673d4", "target_hash": "9390298f3fb0c5b160498935d79cb139aef28e1c47358b4bbba61862b9c26e59", "exact_match": 1.0}
{"doc_id": 16, "doc": {"title": "Unsupervised Learning of Syntactic Structure with Invertible Neural Projections", "abstract": "Unsupervised learning of syntactic structure is typically performed using generative models with discrete latent variables and multinomial parameters. In most cases, these models have not leveraged continuous word representations. In this work, we propose a novel generative model that jointly learns discrete syntactic structure and continuous word representations in an unsupervised fashion by cascading an invertible neural network with a structured generative prior. We show that the invertibility condition allows for efficient exact inference and marginal likelihood computation in our model so long as the prior is well-behaved. In experiments we instantiate our approach with both Markov and tree-structured priors, evaluating on two tasks: part-of-speech (POS) induction, and unsupervised dependency parsing without gold POS annotation. On the Penn Treebank, our Markov-structured model surpasses state-of-the-art results on POS induction. Similarly, we find that our tree-structured model achieves state-of-the-art performance on unsupervised dependency parsing for the difficult training condition where neither gold POS annotation nor punctuation-based constraints are available.", "question": "Do they evaluate only on English datasets?", "answer": "yes", "answer_type": "bool"}, "target": "yes", "arguments": {"gen_args_0": {"arg_0": ["[{\"role\": \"user\", \"content\": \"You are a strict boolean answerer. Only say 'yes' or 'no'.\\nTITLE: Unsupervised Learning of Syntactic Structure with Invertible Neural Projections\\nABSTRACT: Unsupervised learning of syntactic structure is typically performed using generative models with discrete latent variables and multinomial parameters. In most cases, these models have not leveraged continuous word representations. In this work, we propose a novel generative model that jointly learns discrete syntactic structure and continuous word representations in an unsupervised fashion by cascading an invertible neural network with a structured generative prior. We show that the invertibility condition allows for efficient exact inference and marginal likelihood computation in our model so long as the prior is well-behaved. In experiments we instantiate our approach with both Markov and tree-structured priors, evaluating on two tasks: part-of-speech (POS) induction, and unsupervised dependency parsing without gold POS annotation. On the Penn Treebank, our Markov-structured model surpasses state-of-the-art results on POS induction. Similarly, we find that our tree-structured model achieves state-of-the-art performance on unsupervised dependency parsing for the difficult training condition where neither gold POS annotation nor punctuation-based constraints are available.\\nQUESTION: Do they evaluate only on English datasets?\\nANSWER:\"}]"], "arg_1": {"until": ["\n", "."], "max_gen_toks": 5}}}, "resps": [["No"]], "filtered_resps": ["no"], "filter": "get-answer", "metrics": ["exact_match"], "doc_hash": "fcb5580962ea31126c2d9c9c64024d7770b9d5429996dc75ced207da69c2835f", "prompt_hash": "5b0b6c4b91fee9635b905f15a94d30f6d31dfc8dda0b1971d2de96ece91d4e1c", "target_hash": "8a798890fe93817163b10b5f7bd2ca4d25d84c52739a645a889c173eee7d9d3d", "exact_match": 0.0}
{"doc_id": 17, "doc": {"title": "Conflict as an Inverse of Attention in Sequence Relationship", "abstract": "Attention is a very efficient way to model the relationship between two sequences by comparing how similar two intermediate representations are. Initially demonstrated in NMT, it is a standard in all NLU tasks today when efficient interaction between sequences is considered. However, we show that attention, by virtue of its composition, works best only when it is given that there is a match somewhere between two sequences. It does not very well adapt to cases when there is no similarity between two sequences or if the relationship is contrastive. We propose an Conflict model which is very similar to how attention works but which emphasizes mostly on how well two sequences repel each other and finally empirically show how this method in conjunction with attention can boost the overall performance.", "question": "Do they show on which examples how conflict works better than attention?", "answer": "yes", "answer_type": "bool"}, "target": "yes", "arguments": {"gen_args_0": {"arg_0": ["[{\"role\": \"user\", \"content\": \"You are a strict boolean answerer. Only say 'yes' or 'no'.\\nTITLE: Conflict as an Inverse of Attention in Sequence Relationship\\nABSTRACT: Attention is a very efficient way to model the relationship between two sequences by comparing how similar two intermediate representations are. Initially demonstrated in NMT, it is a standard in all NLU tasks today when efficient interaction between sequences is considered. However, we show that attention, by virtue of its composition, works best only when it is given that there is a match somewhere between two sequences. It does not very well adapt to cases when there is no similarity between two sequences or if the relationship is contrastive. We propose an Conflict model which is very similar to how attention works but which emphasizes mostly on how well two sequences repel each other and finally empirically show how this method in conjunction with attention can boost the overall performance.\\nQUESTION: Do they show on which examples how conflict works better than attention?\\nANSWER:\"}]"], "arg_1": {"until": ["\n", "."], "max_gen_toks": 5}}}, "resps": [["No"]], "filtered_resps": ["no"], "filter": "get-answer", "metrics": ["exact_match"], "doc_hash": "3117895a91f90d49ccd58af27e84f8d5203ceb905a24954f9a8976c32f7ae532", "prompt_hash": "d7a0e7c32c27040276f8f172db43d3744f52cccad204ec31f56fd17ab7e1881e", "target_hash": "8a798890fe93817163b10b5f7bd2ca4d25d84c52739a645a889c173eee7d9d3d", "exact_match": 0.0}
{"doc_id": 18, "doc": {"title": "Conflict as an Inverse of Attention in Sequence Relationship", "abstract": "Attention is a very efficient way to model the relationship between two sequences by comparing how similar two intermediate representations are. Initially demonstrated in NMT, it is a standard in all NLU tasks today when efficient interaction between sequences is considered. However, we show that attention, by virtue of its composition, works best only when it is given that there is a match somewhere between two sequences. It does not very well adapt to cases when there is no similarity between two sequences or if the relationship is contrastive. We propose an Conflict model which is very similar to how attention works but which emphasizes mostly on how well two sequences repel each other and finally empirically show how this method in conjunction with attention can boost the overall performance.", "question": "Do they show on which examples how conflict works better than attention?", "answer": "yes", "answer_type": "bool"}, "target": "yes", "arguments": {"gen_args_0": {"arg_0": ["[{\"role\": \"user\", \"content\": \"You are a strict boolean answerer. Only say 'yes' or 'no'.\\nTITLE: Conflict as an Inverse of Attention in Sequence Relationship\\nABSTRACT: Attention is a very efficient way to model the relationship between two sequences by comparing how similar two intermediate representations are. Initially demonstrated in NMT, it is a standard in all NLU tasks today when efficient interaction between sequences is considered. However, we show that attention, by virtue of its composition, works best only when it is given that there is a match somewhere between two sequences. It does not very well adapt to cases when there is no similarity between two sequences or if the relationship is contrastive. We propose an Conflict model which is very similar to how attention works but which emphasizes mostly on how well two sequences repel each other and finally empirically show how this method in conjunction with attention can boost the overall performance.\\nQUESTION: Do they show on which examples how conflict works better than attention?\\nANSWER:\"}]"], "arg_1": {"until": ["\n", "."], "max_gen_toks": 5}}}, "resps": [["No"]], "filtered_resps": ["no"], "filter": "get-answer", "metrics": ["exact_match"], "doc_hash": "3117895a91f90d49ccd58af27e84f8d5203ceb905a24954f9a8976c32f7ae532", "prompt_hash": "d7a0e7c32c27040276f8f172db43d3744f52cccad204ec31f56fd17ab7e1881e", "target_hash": "8a798890fe93817163b10b5f7bd2ca4d25d84c52739a645a889c173eee7d9d3d", "exact_match": 0.0}
{"doc_id": 19, "doc": {"title": "Multilingual Clustering of Streaming News", "abstract": "Clustering news across languages enables efficient media monitoring by aggregating articles from multilingual sources into coherent stories. Doing so in an online setting allows scalable processing of massive news streams. To this end, we describe a novel method for clustering an incoming stream of multilingual documents into monolingual and crosslingual story clusters. Unlike typical clustering approaches that consider a small and known number of labels, we tackle the problem of discovering an ever growing number of cluster labels in an online fashion, using real news datasets in multiple languages. Our method is simple to implement, computationally efficient and produces state-of-the-art results on datasets in German, English and Spanish.", "question": "Do they use graphical models?", "answer": "no", "answer_type": "bool"}, "target": "no", "arguments": {"gen_args_0": {"arg_0": ["[{\"role\": \"user\", \"content\": \"You are a strict boolean answerer. Only say 'yes' or 'no'.\\nTITLE: Multilingual Clustering of Streaming News\\nABSTRACT: Clustering news across languages enables efficient media monitoring by aggregating articles from multilingual sources into coherent stories. Doing so in an online setting allows scalable processing of massive news streams. To this end, we describe a novel method for clustering an incoming stream of multilingual documents into monolingual and crosslingual story clusters. Unlike typical clustering approaches that consider a small and known number of labels, we tackle the problem of discovering an ever growing number of cluster labels in an online fashion, using real news datasets in multiple languages. Our method is simple to implement, computationally efficient and produces state-of-the-art results on datasets in German, English and Spanish.\\nQUESTION: Do they use graphical models?\\nANSWER:\"}]"], "arg_1": {"until": ["\n", "."], "max_gen_toks": 5}}}, "resps": [["No"]], "filtered_resps": ["no"], "filter": "get-answer", "metrics": ["exact_match"], "doc_hash": "d4007fd75591e0a3eb5249724ed976e64a38e1e6c89c4c82329885d1c089219e", "prompt_hash": "23de7fa7fa7b2eb33a42f644c8819cd633933250785a079362056f83e2271c67", "target_hash": "9390298f3fb0c5b160498935d79cb139aef28e1c47358b4bbba61862b9c26e59", "exact_match": 1.0}
